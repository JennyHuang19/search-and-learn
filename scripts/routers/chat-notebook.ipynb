{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d72c8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a10991",
   "metadata": {},
   "source": [
    "# Load in chat data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8df4cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/gma2/jhjenny9/envs/model_serve/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the lmsys/chatbot_arena_conversations dataset from HuggingFace\n",
    "chat_data = load_dataset(\"lmarena-ai/arena-human-preference-55k\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db88da62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57477, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HuggingFace datasets.Dataset doesn't have .head(), so use .select or .to_pandas()\n",
    "# Show first 5 rows as a DataFrame\n",
    "chat_data_df = chat_data.to_pandas()\n",
    "chat_data_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b0e280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>[\"A marriage license is a legal document that ...</td>\n",
       "      <td>[\"A marriage license and a marriage certificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65089</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>[\"explain function calling. how would you call...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96401</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>[\"How can I create a test set for a very rare ...</td>\n",
       "      <td>[\"Creating a test set for a very rare category...</td>\n",
       "      <td>[\"When building a classifier for a very rare c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198779</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-3.5-turbo-0314</td>\n",
       "      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n",
       "      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n",
       "      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             model_a              model_b  \\\n",
       "0   30192  gpt-4-1106-preview           gpt-4-0613   \n",
       "1   53567           koala-13b           gpt-4-0613   \n",
       "2   65089  gpt-3.5-turbo-0613       mistral-medium   \n",
       "3   96401    llama-2-13b-chat  mistral-7b-instruct   \n",
       "4  198779           koala-13b   gpt-3.5-turbo-0314   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  [\"Is it morally right to try to have a certain...   \n",
       "1  [\"What is the difference between marriage lice...   \n",
       "2  [\"explain function calling. how would you call...   \n",
       "3  [\"How can I create a test set for a very rare ...   \n",
       "4  [\"What is the best way to travel from Tel-Aviv...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [\"The question of whether it is morally right ...   \n",
       "1  [\"A marriage license is a legal document that ...   \n",
       "2  [\"Function calling is the process of invoking ...   \n",
       "3  [\"Creating a test set for a very rare category...   \n",
       "4  [\"The best way to travel from Tel Aviv to Jeru...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "1  [\"A marriage license and a marriage certificat...               0   \n",
       "2  [\"Function calling is the process of invoking ...               0   \n",
       "3  [\"When building a classifier for a very rare c...               1   \n",
       "4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "0               0           0  \n",
       "1               1           0  \n",
       "2               0           1  \n",
       "3               0           0  \n",
       "4               1           0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3149ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep games where one of the specified models is the winner\n",
    "target_models = [\n",
    "    \"gpt-4-0125-preview\",\n",
    "    \"gpt-4-1106-preview\",\n",
    "    \"gpt-4-0314\",\n",
    "    \"gpt-4-0613\",\n",
    "    \"qwen1.5-73b-chat\"\n",
    "]\n",
    "\n",
    "# Assuming chat_data_df has a 'winner' column with model names\n",
    "strong_winners = chat_data_df[\n",
    "    ((chat_data_df[\"winner_model_a\"] == 1) & (chat_data_df[\"model_a\"].isin(target_models))) |\n",
    "    ((chat_data_df[\"winner_model_b\"] == 1) & (chat_data_df[\"model_b\"].isin(target_models))) |\n",
    "    ((chat_data_df[\"winner_tie\"] == 1) & (chat_data_df[\"model_a\"].isin(target_models)) | (chat_data_df[\"model_b\"].isin(target_models)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "718805f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15242, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all conversations where the winner is one of the top 5 models. this should, in hopes, provide a good\n",
    "# signal for what a \"good\" answer is.\n",
    "strong_winners.shape # 15242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9b675a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc5eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a92fe79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a8bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb79f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the .out file as a string for later parsing\n",
    "out_path = \"/dccstor/gma2/jhjenny9/search-and-learn/data/beam_numina/pita2240.out\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b56283",
   "metadata": {},
   "source": [
    "#### Get Latencies and Token-Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90577a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def get_beam_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split content into question blocks\n",
    "    # Each block ends with \"Total beam search time: X.XX seconds.\"\n",
    "    blocks = re.split(r'Total beam search time: \\d+\\.\\d+ seconds\\.', content)\n",
    "    \n",
    "    # Remove empty blocks and the last one (which might be incomplete)\n",
    "    blocks = [block.strip() for block in blocks if block.strip()]\n",
    "    \n",
    "    data = []\n",
    "    prev_total_tokens = 0\n",
    "    \n",
    "    for i, block in enumerate(blocks):\n",
    "        # Extract the last \"Total number of tokens generated thus far: X\" before the beam search time\n",
    "        token_matches = re.findall(r'Total number of tokens generated thus far: (\\d+)', block)\n",
    "        \n",
    "        if token_matches:\n",
    "            current_total_tokens = int(token_matches[-1])\n",
    "            \n",
    "            # Calculate tokens for this question (difference from previous)\n",
    "            beam_tc = current_total_tokens - prev_total_tokens\n",
    "            \n",
    "            # Extract beam latency from the next block's start (since we split on it)\n",
    "            if i < len(blocks) - 1:\n",
    "                # Look for the beam search time in the original content\n",
    "                # Find the position of this block in the original content\n",
    "                block_start = content.find(block)\n",
    "                block_end = block_start + len(block)\n",
    "                \n",
    "                # Look for \"Total beam search time:\" after this block\n",
    "                time_match = re.search(r'Total beam search time: (\\d+\\.\\d+) seconds\\.', content[block_end:])\n",
    "                if time_match:\n",
    "                    beam_latency = float(time_match.group(1))\n",
    "                else:\n",
    "                    beam_latency = None\n",
    "            else:\n",
    "                # For the last block, we need to look at the end of the file\n",
    "                time_match = re.search(r'Total beam search time: (\\d+\\.\\d+) seconds\\.$', content)\n",
    "                beam_latency = float(time_match.group(1)) if time_match else None\n",
    "            \n",
    "            # Question ID is 5010 + i\n",
    "            sb_idx = 5010 + i\n",
    "            \n",
    "            data.append({\n",
    "                'sb_idx': sb_idx,\n",
    "                'beam_tc': beam_tc,\n",
    "                'beam_latency': beam_latency\n",
    "            })\n",
    "            \n",
    "            prev_total_tokens = current_total_tokens\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d8734d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sb_idx</th>\n",
       "      <th>beam_tc</th>\n",
       "      <th>beam_latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5010</td>\n",
       "      <td>13276</td>\n",
       "      <td>30.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5011</td>\n",
       "      <td>5643</td>\n",
       "      <td>11.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5012</td>\n",
       "      <td>9461</td>\n",
       "      <td>28.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5013</td>\n",
       "      <td>3837</td>\n",
       "      <td>8.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5014</td>\n",
       "      <td>5405</td>\n",
       "      <td>12.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sb_idx  beam_tc  beam_latency\n",
       "0    5010    13276         30.93\n",
       "1    5011     5643         11.27\n",
       "2    5012     9461         28.21\n",
       "3    5013     3837          8.98\n",
       "4    5014     5405         12.53"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse the file\n",
    "Beam_8_5010_5020 = get_beam_data('/dccstor/gma2/jhjenny9/search-and-learn/Beam-8-5010-5020.out')\n",
    "Beam_8_5010_5020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "365d67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def parse_beam_data_from_filename(file_path):\n",
    "    # Extract beam size and question range from filename\n",
    "    # Example: \"Beam-8-5010-5020.out\" -> beam_size=8, start=5010, end=5020\n",
    "    filename = os.path.basename(file_path)\n",
    "    match = re.match(r'Beam-(\\d+)-(\\d+)-(\\d+)\\.out', filename)\n",
    "    \n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid filename format: {filename}\")\n",
    "    \n",
    "    beam_size = int(match.group(1))\n",
    "    start_idx = int(match.group(2))\n",
    "    end_idx = int(match.group(3))\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split content into question blocks\n",
    "    blocks = re.split(r'Total beam search time: \\d+\\.\\d+ seconds\\.', content)\n",
    "    blocks = [block.strip() for block in blocks if block.strip()]\n",
    "    \n",
    "    data = []\n",
    "    prev_total_tokens = 0\n",
    "    \n",
    "    for i, block in enumerate(blocks):\n",
    "        # Extract the last \"Total number of tokens generated thus far: X\" before the beam search time\n",
    "        token_matches = re.findall(r'Total number of tokens generated thus far: (\\d+)', block)\n",
    "        \n",
    "        if token_matches:\n",
    "            current_total_tokens = int(token_matches[-1])\n",
    "            \n",
    "            # Calculate tokens for this question (difference from previous)\n",
    "            beam_tc = current_total_tokens - prev_total_tokens\n",
    "            \n",
    "            # Extract beam latency from the next block's start\n",
    "            if i < len(blocks) - 1:\n",
    "                block_start = content.find(block)\n",
    "                block_end = block_start + len(block)\n",
    "                \n",
    "                time_match = re.search(r'Total beam search time: (\\d+\\.\\d+) seconds\\.', content[block_end:])\n",
    "                if time_match:\n",
    "                    beam_latency = float(time_match.group(1))\n",
    "                else:\n",
    "                    beam_latency = None\n",
    "            else:\n",
    "                # For the last block, look at the end of the file\n",
    "                time_match = re.search(r'Total beam search time: (\\d+\\.\\d+) seconds\\.$', content)\n",
    "                beam_latency = float(time_match.group(1)) if time_match else None\n",
    "            \n",
    "            # Question ID is start_idx + i\n",
    "            sb_idx = start_idx + i\n",
    "            \n",
    "            data.append({\n",
    "                'sb_idx': sb_idx,\n",
    "                'beam_tc': beam_tc,\n",
    "                'beam_latency': beam_latency,\n",
    "                'N': beam_size\n",
    "            })\n",
    "            \n",
    "            prev_total_tokens = current_total_tokens\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def process_all_beam_files(directory_path):\n",
    "    # Find all Beam-*.out files\n",
    "    pattern = os.path.join(directory_path, \"Beam-*.out\")\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for file_path in sorted(files):\n",
    "        try:\n",
    "            df = parse_beam_data_from_filename(file_path)\n",
    "            all_data.append(df)\n",
    "            print(f\"Processed: {os.path.basename(file_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    if all_data:\n",
    "        # Combine all DataFrames\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b12409e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: Beam-16-10000-11000.out\n",
      "Processed: Beam-16-11000-12000.out\n",
      "Processed: Beam-16-12000-13000.out\n",
      "Processed: Beam-16-13000-14000.out\n",
      "Processed: Beam-16-14000-15000.out\n",
      "Processed: Beam-16-15000-16000.out\n",
      "Processed: Beam-16-16000-17000.out\n",
      "Processed: Beam-16-17000-18000.out\n",
      "Processed: Beam-16-18000-19000.out\n",
      "Processed: Beam-16-19000-20000.out\n",
      "Processed: Beam-16-20000-21000.out\n",
      "Processed: Beam-16-21000-22000.out\n",
      "Processed: Beam-16-22000-23000.out\n",
      "Processed: Beam-16-23000-24000.out\n",
      "Processed: Beam-16-24000-25000.out\n",
      "Processed: Beam-16-25000-26000.out\n",
      "Processed: Beam-16-26000-27000.out\n",
      "Processed: Beam-16-27000-28000.out\n",
      "Processed: Beam-16-28000-29000.out\n",
      "Processed: Beam-16-29000-30000.out\n",
      "Processed: Beam-16-30000-32000.out\n",
      "Processed: Beam-16-32000-34000.out\n",
      "Processed: Beam-16-34000-36000.out\n",
      "Processed: Beam-16-36000-38000.out\n",
      "Processed: Beam-16-38000-40000.out\n",
      "Processed: Beam-16-40000-42000.out\n",
      "Processed: Beam-16-42000-44000.out\n",
      "Processed: Beam-16-44000-46000.out\n",
      "Processed: Beam-16-46000-48000.out\n",
      "Processed: Beam-16-48000-50000.out\n",
      "Processed: Beam-16-5000-6000.out\n",
      "Processed: Beam-16-6000-7000.out\n",
      "Processed: Beam-16-7000-8000.out\n",
      "Processed: Beam-16-8000-9000.out\n",
      "Processed: Beam-16-9000-10000.out\n",
      "Processed: Beam-32-10000-11000.out\n",
      "Processed: Beam-32-11000-12000.out\n",
      "Processed: Beam-32-12000-13000.out\n",
      "Processed: Beam-32-13000-14000.out\n",
      "Processed: Beam-32-14000-15000.out\n",
      "Processed: Beam-32-15000-16000.out\n",
      "Processed: Beam-32-16000-17000.out\n",
      "Processed: Beam-32-17000-18000.out\n",
      "Processed: Beam-32-18000-19000.out\n",
      "Processed: Beam-32-19000-20000.out\n",
      "Processed: Beam-32-20000-21000.out\n",
      "Processed: Beam-32-21000-22000.out\n",
      "Processed: Beam-32-22000-23000.out\n",
      "Processed: Beam-32-23000-24000.out\n",
      "Processed: Beam-32-24000-25000.out\n",
      "Processed: Beam-32-25000-26000.out\n",
      "Processed: Beam-32-26000-27000.out\n",
      "Processed: Beam-32-27000-28000.out\n",
      "Processed: Beam-32-28000-29000.out\n",
      "Processed: Beam-32-29000-30000.out\n",
      "Processed: Beam-32-5000-6000.out\n",
      "Processed: Beam-32-6000-7000.out\n",
      "Processed: Beam-32-7000-8000.out\n",
      "Processed: Beam-32-8000-9000.out\n",
      "Processed: Beam-32-9000-10000.out\n",
      "Processed: Beam-8-11000-12000.out\n",
      "Processed: Beam-8-12000-13000.out\n",
      "Processed: Beam-8-13000-14000.out\n",
      "Processed: Beam-8-14000-15000.out\n",
      "Processed: Beam-8-15000-16000.out\n",
      "Processed: Beam-8-16000-17000.out\n",
      "Processed: Beam-8-17000-18000.out\n",
      "Processed: Beam-8-18000-19000.out\n",
      "Processed: Beam-8-19000-20000.out\n",
      "Processed: Beam-8-20000-21000.out\n",
      "Processed: Beam-8-21000-22000.out\n",
      "Processed: Beam-8-22000-23000.out\n",
      "Processed: Beam-8-23000-24000.out\n",
      "Processed: Beam-8-24000-25000.out\n",
      "Processed: Beam-8-25000-26000.out\n",
      "Processed: Beam-8-26000-27000.out\n",
      "Processed: Beam-8-27000-28000.out\n",
      "Processed: Beam-8-28000-29000.out\n",
      "Processed: Beam-8-29000-30000.out\n",
      "Processed: Beam-8-30000-32000.out\n",
      "Processed: Beam-8-32000-34000.out\n",
      "Processed: Beam-8-34000-36000.out\n",
      "Processed: Beam-8-36000-38000.out\n",
      "Processed: Beam-8-38000-40000.out\n",
      "Processed: Beam-8-40000-42000.out\n",
      "Processed: Beam-8-42000-44000.out\n",
      "Processed: Beam-8-44000-46000.out\n",
      "Processed: Beam-8-46000-48000.out\n",
      "Processed: Beam-8-48000-50000.out\n",
      "Processed: Beam-8-5010-5020.out\n",
      "\n",
      "Combined DataFrame:\n",
      "       sb_idx  beam_tc  beam_latency   N\n",
      "0       10000    17263         24.99  16\n",
      "1       10001     4867          8.25  16\n",
      "2       10002    22265         41.00  16\n",
      "3       10003    13514         21.97  16\n",
      "4       10004    12656         18.20  16\n",
      "...       ...      ...           ...  ..\n",
      "63031    5015    20909         61.46   8\n",
      "63032    5016     7763         14.00   8\n",
      "63033    5017     6097         11.38   8\n",
      "63034    5018    12346         28.72   8\n",
      "63035    5019     5702          9.48   8\n",
      "\n",
      "[63036 rows x 4 columns]\n",
      "\n",
      "Total rows: 63036\n"
     ]
    }
   ],
   "source": [
    "# Process all files in the directory\n",
    "directory = \"/dccstor/gma2/jhjenny9/search-and-learn\"\n",
    "df = process_all_beam_files(directory)\n",
    "\n",
    "if not df.empty:\n",
    "    print(\"\\nCombined DataFrame:\")\n",
    "    print(df)\n",
    "    print(f\"\\nTotal rows: {len(df)}\")\n",
    "else:\n",
    "    print(\"No data found or error occurred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04786764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">beam_latency</th>\n",
       "      <th colspan=\"2\" halign=\"left\">beam_tc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>58.915149</td>\n",
       "      <td>15972</td>\n",
       "      <td>32975.299587</td>\n",
       "      <td>15972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>36.347403</td>\n",
       "      <td>22890</td>\n",
       "      <td>19570.513718</td>\n",
       "      <td>22890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>77.906351</td>\n",
       "      <td>24174</td>\n",
       "      <td>44421.821378</td>\n",
       "      <td>24174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   beam_latency              beam_tc       \n",
       "           mean  count          mean  count\n",
       "N                                          \n",
       "8     58.915149  15972  32975.299587  15972\n",
       "16    36.347403  22890  19570.513718  22890\n",
       "32    77.906351  24174  44421.821378  24174"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"N\")[[\"beam_latency\", \"beam_tc\"]].agg([\"mean\", \"count\"]) # debug this on a smaller dataset (N=8,16,32) to make sure the data are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2a8e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bert_features_path = \"/dccstor/gma2/jhjenny9/search-and-learn/feature-data/bert-features/16000/df_test_16000.csv\"\n",
    "bert_df = pd.read_csv(bert_features_path)\n",
    "bert_df.head()\n",
    "\n",
    "bert_df_beam = bert_df[bert_df[\"method_beam_search\"] == True]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "427f7e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5282, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_df_beam.shape # ask the parse_beam_data_from_filename script to only retrieve the rows where sb_idx matches one of the values in method_beam_search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30c7f4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 31)\n"
     ]
    }
   ],
   "source": [
    "# read in this file.\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "beam_completions_path = \"/dccstor/gma2/jhjenny9/search-and-learn/data/Numina_Beam/Numina_beam_search_4_4_40_fourth_chunk/beam_search_completions.jsonl\"\n",
    "with open(beam_completions_path, \"r\") as f:\n",
    "    completions = [json.loads(line) for line in f]\n",
    "df_completions = pd.DataFrame(completions)\n",
    "print(df_completions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b536c6bb",
   "metadata": {},
   "source": [
    "#### Beam Times for Small vs. Large N Beam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c5844cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def extract_average_beam_search_time(log_file_path):\n",
    "    \"\"\"\n",
    "    Extract all \"Total beam search time:\" values from a log file and calculate the average.\n",
    "    \n",
    "    Args:\n",
    "        log_file_path (str): Path to the log file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing:\n",
    "            - 'times': List of all beam search times in seconds\n",
    "            - 'average': Average beam search time in seconds\n",
    "            - 'count': Number of beam search operations\n",
    "            - 'min': Minimum beam search time\n",
    "            - 'max': Maximum beam search time\n",
    "            - 'std': Standard deviation of beam search times\n",
    "    \"\"\"\n",
    "    # Pattern to match \"Total beam search time: X.XX seconds.\"\n",
    "    pattern = r'Total beam search time: ([\\d.]+) seconds\\.'\n",
    "    \n",
    "    times = []\n",
    "    \n",
    "    try:\n",
    "        with open(log_file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                match = re.search(pattern, line)\n",
    "                if match:\n",
    "                    time_value = float(match.group(1))\n",
    "                    times.append(time_value)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{log_file_path}' not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None\n",
    "    \n",
    "    if not times:\n",
    "        print(\"No beam search times found in the log file.\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate statistics\n",
    "    times_array = np.array(times)\n",
    "    result = {\n",
    "        'times': times,\n",
    "        'average': np.mean(times_array),\n",
    "        'count': len(times),\n",
    "        'min': np.min(times_array),\n",
    "        'max': np.max(times_array),\n",
    "        'std': np.std(times_array)\n",
    "    }\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35795054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam Time for N=128\n",
      "Found 10 beam search operations\n",
      "Average time: 388.20 seconds\n",
      "Min time: 133.41 seconds\n",
      "Max time: 616.34 seconds\n",
      "Standard deviation: 123.20 seconds\n",
      "\n",
      "All times: [297.57, 403.46, 403.98, 415.98, 133.41, 616.34, 359.12, 514.49, 431.28, 306.37]\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "result = extract_average_beam_search_time('/dccstor/gma2/jhjenny9/search-and-learn/large-beam-2.out')\n",
    "\n",
    "print(f\"Beam Time for N=128\")\n",
    "print(f\"Found {result['count']} beam search operations\")\n",
    "print(f\"Average time: {result['average']:.2f} seconds\")\n",
    "print(f\"Min time: {result['min']:.2f} seconds\")\n",
    "print(f\"Max time: {result['max']:.2f} seconds\")\n",
    "print(f\"Standard deviation: {result['std']:.2f} seconds\")\n",
    "print(f\"\\nAll times: {result['times']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "443cdfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam Time for N=32\n",
      "Found 100 beam search operations\n",
      "Average time: 81.80 seconds\n",
      "Min time: 10.66 seconds\n",
      "Max time: 264.00 seconds\n",
      "Standard deviation: 57.81 seconds\n",
      "\n",
      "All times: [24.76, 19.27, 99.12, 95.19, 26.03, 78.23, 13.39, 31.76, 134.76, 11.11, 56.41, 121.37, 109.11, 18.25, 264.0, 23.91, 62.78, 65.45, 64.4, 48.95, 118.92, 120.7, 24.39, 70.16, 160.42, 25.36, 167.64, 103.07, 88.3, 13.57, 27.39, 85.95, 21.11, 37.36, 41.51, 78.14, 77.44, 123.13, 122.26, 24.13, 20.46, 99.25, 21.17, 75.69, 24.67, 15.8, 19.05, 43.24, 115.7, 135.53, 67.32, 125.27, 73.35, 81.99, 224.16, 52.6, 103.65, 67.62, 130.81, 175.54, 21.22, 120.32, 176.65, 16.03, 105.63, 90.8, 23.85, 31.28, 36.32, 26.99, 91.97, 111.23, 217.35, 101.09, 35.78, 12.05, 51.53, 110.73, 189.84, 19.03, 11.21, 144.93, 16.41, 42.49, 98.03, 197.02, 102.67, 90.83, 139.4, 173.64, 77.79, 120.7, 82.18, 32.01, 40.53, 134.08, 104.03, 177.38, 222.48, 10.66]\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "result = extract_average_beam_search_time('/dccstor/gma2/jhjenny9/search-and-learn/large-beam.out')\n",
    "\n",
    "print(f\"Beam Time for N=32\")\n",
    "print(f\"Found {result['count']} beam search operations\")\n",
    "print(f\"Average time: {result['average']:.2f} seconds\")\n",
    "print(f\"Min time: {result['min']:.2f} seconds\")\n",
    "print(f\"Max time: {result['max']:.2f} seconds\")\n",
    "print(f\"Standard deviation: {result['std']:.2f} seconds\")\n",
    "print(f\"\\nAll times: {result['times']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6cadf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23033ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_serve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
