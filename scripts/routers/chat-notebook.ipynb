{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d72c8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a10991",
   "metadata": {},
   "source": [
    "# Load in chat data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8df4cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/gma2/jhjenny9/envs/model_serve/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the lmsys/chatbot_arena_conversations dataset from HuggingFace\n",
    "chat_data = load_dataset(\"lmarena-ai/arena-human-preference-55k\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db88da62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57477, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HuggingFace datasets.Dataset doesn't have .head(), so use .select or .to_pandas()\n",
    "# Show first 5 rows as a DataFrame\n",
    "chat_data_df = chat_data.to_pandas()\n",
    "chat_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14b0e280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>[\"A marriage license is a legal document that ...</td>\n",
       "      <td>[\"A marriage license and a marriage certificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65089</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>[\"explain function calling. how would you call...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96401</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>[\"How can I create a test set for a very rare ...</td>\n",
       "      <td>[\"Creating a test set for a very rare category...</td>\n",
       "      <td>[\"When building a classifier for a very rare c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198779</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-3.5-turbo-0314</td>\n",
       "      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n",
       "      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n",
       "      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             model_a              model_b  \\\n",
       "0   30192  gpt-4-1106-preview           gpt-4-0613   \n",
       "1   53567           koala-13b           gpt-4-0613   \n",
       "2   65089  gpt-3.5-turbo-0613       mistral-medium   \n",
       "3   96401    llama-2-13b-chat  mistral-7b-instruct   \n",
       "4  198779           koala-13b   gpt-3.5-turbo-0314   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  [\"Is it morally right to try to have a certain...   \n",
       "1  [\"What is the difference between marriage lice...   \n",
       "2  [\"explain function calling. how would you call...   \n",
       "3  [\"How can I create a test set for a very rare ...   \n",
       "4  [\"What is the best way to travel from Tel-Aviv...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [\"The question of whether it is morally right ...   \n",
       "1  [\"A marriage license is a legal document that ...   \n",
       "2  [\"Function calling is the process of invoking ...   \n",
       "3  [\"Creating a test set for a very rare category...   \n",
       "4  [\"The best way to travel from Tel Aviv to Jeru...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "1  [\"A marriage license and a marriage certificat...               0   \n",
       "2  [\"Function calling is the process of invoking ...               0   \n",
       "3  [\"When building a classifier for a very rare c...               1   \n",
       "4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "0               0           0  \n",
       "1               1           0  \n",
       "2               0           1  \n",
       "3               0           0  \n",
       "4               1           0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3149ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep games where one of the specified models is the winner\n",
    "target_models = [\n",
    "    \"gpt-4-0125-preview\",\n",
    "    \"gpt-4-1106-preview\",\n",
    "    \"gpt-4-0314\",\n",
    "    \"gpt-4-0613\",\n",
    "    \"qwen1.5-73b-chat\"\n",
    "]\n",
    "\n",
    "# Assuming chat_data_df has a 'winner' column with model names\n",
    "strong_winners = chat_data_df[\n",
    "    ((chat_data_df[\"winner_model_a\"] == 1) & (chat_data_df[\"model_a\"].isin(target_models))) |\n",
    "    ((chat_data_df[\"winner_model_b\"] == 1) & (chat_data_df[\"model_b\"].isin(target_models))) |\n",
    "    ((chat_data_df[\"winner_tie\"] == 1) & (chat_data_df[\"model_a\"].isin(target_models)) | (chat_data_df[\"model_b\"].isin(target_models)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "718805f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15242, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all conversations where the winner is one of the top 5 models. this should, in hopes, provide a good\n",
    "# signal for what a \"good\" answer is.\n",
    "strong_winners.shape # 15242"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b2e9e",
   "metadata": {},
   "source": [
    "# Visualize Distribution of Rouge Scores For Nonchosen Answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc9b675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rouge_score\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "def rouge_similarity(hypotheis, reference):\n",
    "    return scorer.score(hypotheis, reference)[\"rougeL\"].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29460afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7692307692307692\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "hyp = \"the cat sat on the mat\"\n",
    "ref = \"the cat is sitting on the mat\"\n",
    "\n",
    "print(rouge_similarity(hyp, ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5afe84a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_data_df_first_100 = chat_data_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "934b3c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2300441/922331732.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chat_data_df_first_100[[\"rougeL_f\"]] = chat_data_df_first_100.apply(compute_rouge, axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "      <th>rougeL_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.172600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>[\"A marriage license is a legal document that ...</td>\n",
       "      <td>[\"A marriage license and a marriage certificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.284229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65089</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>[\"explain function calling. how would you call...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.332604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96401</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>[\"How can I create a test set for a very rare ...</td>\n",
       "      <td>[\"Creating a test set for a very rare category...</td>\n",
       "      <td>[\"When building a classifier for a very rare c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.240385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198779</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-3.5-turbo-0314</td>\n",
       "      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n",
       "      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n",
       "      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             model_a              model_b  \\\n",
       "0   30192  gpt-4-1106-preview           gpt-4-0613   \n",
       "1   53567           koala-13b           gpt-4-0613   \n",
       "2   65089  gpt-3.5-turbo-0613       mistral-medium   \n",
       "3   96401    llama-2-13b-chat  mistral-7b-instruct   \n",
       "4  198779           koala-13b   gpt-3.5-turbo-0314   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  [\"Is it morally right to try to have a certain...   \n",
       "1  [\"What is the difference between marriage lice...   \n",
       "2  [\"explain function calling. how would you call...   \n",
       "3  [\"How can I create a test set for a very rare ...   \n",
       "4  [\"What is the best way to travel from Tel-Aviv...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [\"The question of whether it is morally right ...   \n",
       "1  [\"A marriage license is a legal document that ...   \n",
       "2  [\"Function calling is the process of invoking ...   \n",
       "3  [\"Creating a test set for a very rare category...   \n",
       "4  [\"The best way to travel from Tel Aviv to Jeru...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "1  [\"A marriage license and a marriage certificat...               0   \n",
       "2  [\"Function calling is the process of invoking ...               0   \n",
       "3  [\"When building a classifier for a very rare c...               1   \n",
       "4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  rougeL_f  \n",
       "0               0           0  0.172600  \n",
       "1               1           0  0.284229  \n",
       "2               0           1  0.332604  \n",
       "3               0           0  0.240385  \n",
       "4               1           0  0.300268  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Example: assuming your DataFrame is called df with columns response_a, response_b\n",
    "# df = pd.read_csv(\"your_file.csv\")\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "def compute_rouge(row):\n",
    "    scores = scorer.score(row[\"response_a\"], row[\"response_b\"])\n",
    "    return pd.Series({\n",
    "        \"rougeL_f\": scores[\"rougeL\"].fmeasure\n",
    "    })\n",
    "\n",
    "# Apply across rows\n",
    "chat_data_df_first_100[[\"rougeL_f\"]] = chat_data_df_first_100.apply(compute_rouge, axis=1)\n",
    "\n",
    "chat_data_df_first_100.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3d88e5",
   "metadata": {},
   "source": [
    "Distribution of rouge scores between preferred vs. unpreferred response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85870e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAq4AAAHWCAYAAAC2Zgs3AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAPdZJREFUeJzt3Xd4FPX+/v97U3aTQEILEIIBQqiCgtIOgoB0UBTLBxVB4ICco1jBhpwj1S6IBcVzVLAcAfEg+lVAujT7AUQNJRRDJAQSkRAS0vb9+8Nf9nJJIVm2ZMLzcV176czOzus1+94JdyYzszZjjBEAAABQyQUFugEAAACgPAiuAAAAsASCKwAAACyB4AoAAABLILgCAADAEgiuAAAAsASCKwAAACyB4AoAAABLILgCAADAEgiuAEo0bdo02Ww2v9Tq1auXevXq5ZreuHGjbDabPvzwQ7/UHz16tJo0aeKXWp7KysrSuHHjFBMTI5vNpvvvvz/QLQGA3xFcgQvAwoULZbPZXI+wsDDFxsZqwIABeumll3Tq1Cmv1Dly5IimTZumHTt2eGV93lSZeyuPJ598UgsXLtSdd96pd999VyNHjix12SZNmriNd7Vq1dS5c2e98847fuzY93bt2qWbbrpJjRs3VlhYmBo2bKh+/frp5ZdfDnRrAHzEZowxgW4CgG8tXLhQY8aM0YwZMxQfH6/8/HwdPXpUGzdu1Jo1a9SoUSN98sknuvTSS12vKSgoUEFBgcLCwspd57vvvlOnTp20YMECjR49utyvy8vLkyTZ7XZJfxxxveqqq7R06VLddNNN5V6Pp73l5+fL6XTK4XB4pZYv/OUvf1FISIi2bNlyzmWbNGmiWrVqadKkSZKk1NRUvfHGG9q7d6/+9a9/6Y477vB1uz63bds2XXXVVWrUqJFGjRqlmJgYHT58WF999ZX279+vpKSkQLcIwAdCAt0AAP8ZNGiQOnbs6JqePHmy1q9fr2uuuUbXXnutEhMTFR4eLkkKCQlRSIhvf0RkZ2crIiLCFVgDJTQ0NKD1y+PYsWO6+OKLy718w4YNNWLECNf06NGj1bRpU73wwgtVIrg+8cQTqlGjhr799lvVrFnT7bljx475tZeizzEA3+NUAeAC17t3b/3zn//UL7/8ovfee881v6RzXNesWaPu3burZs2aql69ulq2bKnHHntM0h9HSTt16iRJGjNmjOvP1AsXLpT0x3msbdu21ffff68ePXooIiLC9dqzz3EtUlhYqMcee0wxMTGqVq2arr32Wh0+fNhtmSZNmpR4dPfP6zxXbyWd43r69GlNmjRJcXFxcjgcatmypZ5//nmd/Ucqm82mu+++W8uXL1fbtm3lcDjUpk0brVq1quQ3/CzHjh3T2LFjVb9+fYWFhaldu3Z6++23Xc8Xne978OBBffbZZ67eDx06VK71F6lbt65atWql/fv3V3g7Dx065PZ+nb3906ZNc5u3ceNGdezYUWFhYUpISNDrr79e6jnT7733njp06KDw8HDVrl1bt9xyS7ExLsn+/fvVpk2bYqFVkurVq1dinc6dOysiIkK1atVSjx49tHr1ardlXn31VbVp00YOh0OxsbGaMGGCfv/9d7dlyvoc5+bmaurUqWrWrJkcDofi4uL08MMPKzc3120dZe1HAMrGEVcAGjlypB577DGtXr261KNxP/30k6655hpdeumlmjFjhhwOh5KSkrR161ZJUuvWrTVjxgw9/vjjGj9+vK688kpJ0hVXXOFaR0ZGhgYNGqRbbrlFI0aMUP369cvs64knnpDNZtMjjzyiY8eOae7cuerbt6927NjhOjJcHuXp7c+MMbr22mu1YcMGjR07Vu3bt9fnn3+uhx56SL/++qteeOEFt+W3bNmiZcuW6a677lJkZKReeukl3XjjjUpOTladOnVK7SsnJ0e9evVSUlKS7r77bsXHx2vp0qUaPXq0fv/9d913331q3bq13n33XT3wwAO66KKLXH/+r1u3brm3X/rj1I+UlBTVqlXL4+0sj+3bt2vgwIFq0KCBpk+frsLCQs2YMaPEfp944gn985//1LBhwzRu3DgdP35cL7/8snr06KHt27eXGEqLNG7cWF9++aV+/PFHtW3btsyepk+frmnTpumKK67QjBkzZLfb9fXXX2v9+vXq37+/pD9+UZs+fbr69u2rO++8U3v27NFrr72mb7/9Vlu3bnU7Kl/S59jpdOraa6/Vli1bNH78eLVu3Vq7du3SCy+8oL1792r58uWSzr0fATgHA6DKW7BggZFkvv3221KXqVGjhrnssstc01OnTjV//hHxwgsvGEnm+PHjpa7j22+/NZLMggULij3Xs2dPI8nMnz+/xOd69uzpmt6wYYORZBo2bGgyMzNd8z/44AMjybz44ouueY0bNzajRo065zrL6m3UqFGmcePGrunly5cbSWbWrFluy910003GZrOZpKQk1zxJxm63u83buXOnkWRefvnlYrX+bO7cuUaSee+991zz8vLyTNeuXU316tXdtr1x48bm6quvLnN9f162f//+5vjx4+b48eNm165dZuTIkUaSmTBhQoW38+DBg6W+d5LM1KlTXdNDhgwxERER5tdff3XN27dvnwkJCXH7PB06dMgEBwebJ554wm19u3btMiEhIcXmn2316tUmODjYBAcHm65du5qHH37YfP755yYvL89tuX379pmgoCBz/fXXm8LCQrfnnE6nMcaYY8eOGbvdbvr37++2zCuvvGIkmbfeess1r7TP8bvvvmuCgoLM5s2b3ebPnz/fSDJbt241xpRvPwJQOk4VACBJql69epl3Fyg6+vXxxx/L6XR6VMPhcGjMmDHlXv72229XZGSka/qmm25SgwYNtGLFCo/ql9eKFSsUHByse++9123+pEmTZIzRypUr3eb37dtXCQkJrulLL71UUVFROnDgwDnrxMTE6NZbb3XNCw0N1b333qusrCx98cUXHm/D6tWrVbduXdWtW1eXXHKJ3n33XY0ZM0bPPfecx9t5LoWFhVq7dq2GDh2q2NhY1/xmzZpp0KBBbssuW7ZMTqdTw4YNU3p6uusRExOj5s2ba8OGDWXW6tevn7788ktde+212rlzp5599lkNGDBADRs21CeffOJabvny5XI6nXr88ccVFOT+T17RqQtr165VXl6e7r//frdl7rjjDkVFRemzzz5ze11Jn+OlS5eqdevWatWqldv29O7dW5Jc2+ON/Qi4kBFcAUj64z6hfw6JZ7v55pvVrVs3jRs3TvXr19ctt9yiDz74oEL/+DZs2LBCF2I1b97cbdpms6lZs2YVPr+zon755RfFxsYWez9at27tev7PGjVqVGwdtWrV0okTJ85Zp3nz5sUCVWl1KqJLly5as2aNVq1apeeff141a9bUiRMn3N7/im7nuRw7dkw5OTlq1qxZsefOnrdv3z4ZY9S8eXNXwC56JCYmlusCq06dOmnZsmU6ceKEvvnmG02ePFmnTp3STTfdpJ9//lnSH+fCBgUFlXlhW9F2tmzZ0m2+3W5X06ZNi70PJX2O9+3bp59++qnYtrRo0cL13kje2Y+ACxnnuAJQSkqKTp48WWLgKBIeHq5NmzZpw4YN+uyzz7Rq1SotWbJEvXv31urVqxUcHHzOOhU5L7W8SvuShMLCwnL15A2l1TEBvNtgdHS0+vbtK0kaMGCAWrVqpWuuuUYvvviiJk6cWKF1lfUee8rpdMpms2nlypUlvn/Vq1cv97rsdrs6deqkTp06qUWLFhozZoyWLl2qqVOnetxfWUr6HDudTl1yySWaM2dOia+Ji4tzvfZ89yPgQkZwBaB3331X0h8BpyxBQUHq06eP+vTpozlz5ujJJ5/UlClTtGHDBvXt29fr37S1b98+t2ljjJKSktzuN1urVq1iV35LfxxFa9q0qWu6Ir01btxYa9eu1alTp9yORu7evdv1vDc0btxYP/zwg5xOp9tRV2/XkaSrr75aPXv21JNPPqm//e1vqlatWrm3s+iCrrPf57OPRNarV09hYWEl3kP17HkJCQkyxig+Pt51VNIbim73lpqa6qrjdDr1888/q3379iW+pmg79+zZ4/aZycvL08GDB12/AJQlISFBO3fuVJ8+fc75WTvXfgSgdJwqAFzg1q9fr5kzZyo+Pl633XZbqcv99ttvxeYVBYGi2/1Uq1ZNUvGA46l33nnH7bzbDz/8UKmpqW7nSyYkJOirr75yfYmBJH366afFbqlUkd4GDx6swsJCvfLKK27zX3jhBdlstmLna3pq8ODBOnr0qJYsWeKaV1BQoJdfflnVq1dXz549vVKnyCOPPKKMjAz9+9//dtUvz3ZGRUUpOjpamzZtclvu1VdfdZsODg5W3759tXz5ch05csQ1Pykpqdj5sjfccIOCg4M1ffr0YkemjTHKyMgoc1s2bNhQ4hHtovOfi/7sP3ToUAUFBWnGjBnF/hxf9Pq+ffvKbrfrpZdeclvnm2++qZMnT+rqq68usxdJGjZsmH799VfXe/tnOTk5On36tKTy7UcASscRV+ACsnLlSu3evVsFBQVKS0vT+vXrtWbNGjVu3FiffPJJmd+SNWPGDG3atElXX321GjdurGPHjunVV1/VRRddpO7du0v6I0TWrFlT8+fPV2RkpKpVq6YuXbooPj7eo35r166t7t27a8yYMUpLS9PcuXPVrFkzt1t2jRs3Th9++KEGDhyoYcOGaf/+/XrvvffcLpaqaG9DhgzRVVddpSlTpujQoUNq166dVq9erY8//lj3339/sXV7avz48Xr99dc1evRoff/992rSpIk+/PBDbd26VXPnzi3znGNPDBo0SG3bttWcOXM0YcKECm3nuHHj9PTTT2vcuHHq2LGjNm3apL179xarMW3aNK1evVrdunXTnXfe6QrGbdu2dfu63YSEBM2aNUuTJ0/WoUOHNHToUEVGRurgwYP66KOPNH78eD344IOlbss999yj7OxsXX/99WrVqpXy8vK0bds2LVmyRE2aNHFdPNWsWTNNmTJFM2fO1JVXXqkbbrhBDodD3377rWJjY/XUU0+pbt26mjx5sqZPn66BAwfq2muv1Z49e/Tqq6+qU6dObl/kUJqRI0fqgw8+0N///ndt2LBB3bp1U2FhoXbv3q0PPvhAn3/+uTp27Fiu/QhAGQJ0NwMAflR0O6yih91uNzExMaZfv37mxRdfdLvtUpGzb4e1bt06c91115nY2Fhjt9tNbGysufXWW83evXvdXvfxxx+biy++2HX7o6JbKPXs2dO0adOmxP5Kux3WokWLzOTJk029evVMeHi4ufrqq80vv/xS7PWzZ882DRs2NA6Hw3Tr1s189913xdZZVm9n3w7LGGNOnTplHnjgARMbG2tCQ0NN8+bNzXPPPee6hVIRnXWLqSKl3abrbGlpaWbMmDEmOjra2O12c8kll5R426mK3g6rtGUXLlzotu3l3c7s7GwzduxYU6NGDRMZGWmGDRtmjh07Vux2WMb88Vm57LLLjN1uNwkJCeaNN94wkyZNMmFhYcX6+e9//2u6d+9uqlWrZqpVq2ZatWplJkyYYPbs2VPmNq5cudL89a9/Na1atTLVq1c3drvdNGvWzNxzzz0mLS2t2PJvvfWWueyyy4zD4TC1atUyPXv2NGvWrHFb5pVXXjGtWrUyoaGhpn79+ubOO+80J06ccFumrM9xXl6eeeaZZ0ybNm1cdTp06GCmT59uTp486XpvyrMfASiZzZgAXj0AALggDB06VD/99FOx85YBoCI4xxUA4FU5OTlu0/v27dOKFStK/FpfAKgIjrgCALyqQYMGGj16tOseqK+99ppyc3O1ffv2YvfmBYCK4OIsAIBXDRw4UIsWLdLRo0flcDjUtWtXPfnkk4RWAOeNI64AAACwBM5xBQAAgCUQXAEAAGAJVf4cV6fTqSNHjigyMtLrX0cJAACA82eM0alTpxQbG+v2Fdhnq/LB9ciRI4qLiwt0GwAAADiHw4cP66KLLir1+SofXIu+MvHw4cOKioryeb38/HytXr1a/fv3V2hoqM/rwfsYQ+tjDK2N8bM+xtD6/D2GmZmZiouLO+dXXVf54Fp0ekBUVJTfgmtERISioqLYWS2KMbQ+xtDaGD/rYwytL1BjeK7TOrk4CwAAAJZAcAUAAIAlEFwBAABgCQRXAAAAWALBFQAAAJZAcAUAAIAlEFwBAABgCQRXAAAAWALBFQAAAJZAcAUAAIAlEFwBAABgCQRXAAAAWALBFQAAAJZAcAUAAIAlhAS6AaCy2rlzp4KC/PO7XXR0tBo1auSXWgAAWBXBFThLSkqKJKlHjx7KycnxS82w8Ajt2Z1IeAUAoAwEV+AsGRkZkqTaA+9RYVSsz+vlZxxWxqezlZ6eTnAFAKAMBFegFKG1GyokOiHQbQAAgP8fF2cBAADAEgiuAAAAsASCKwAAACyB4AoAAABLILgCAADAEgiuAAAAsASCKwAAACyB4AoAAABLILgCAADAEgiuAAAAsASCKwAAACyB4AoAAABLILgCAADAEgiuAAAAsASCKwAAACyB4AoAAABLILgCAADAEgiuAAAAsASCKwAAACyB4AoAAABLILgCAADAEgiuAAAAsASCKwAAACyB4AoAAABLILgCAADAEgiuAAAAsASCKwAAACwhoMH1qaeeUqdOnRQZGal69epp6NCh2rNnj9syZ86c0YQJE1SnTh1Vr15dN954o9LS0gLUMQAAAAIloMH1iy++0IQJE/TVV19pzZo1ys/PV//+/XX69GnXMg888ID+3//7f1q6dKm++OILHTlyRDfccEMAuwYAAEAghASy+KpVq9ymFy5cqHr16un7779Xjx49dPLkSb355pt6//331bt3b0nSggUL1Lp1a3311Vf6y1/+Umydubm5ys3NdU1nZmZKkvLz85Wfn+/DrZGrzp//C+txOp2SJEeITSbY+LyeLcSm8PBwOZ1OPjdewn5obYyf9TGG1ufvMSxvHZsxxvf/MpdTUlKSmjdvrl27dqlt27Zav369+vTpoxMnTqhmzZqu5Ro3bqz7779fDzzwQLF1TJs2TdOnTy82//3331dERIQv2wcAAIAHsrOzNXz4cJ08eVJRUVGlLhfQI65/5nQ6df/996tbt25q27atJOno0aOy2+1uoVWS6tevr6NHj5a4nsmTJ2vixImu6czMTMXFxal///5lvhHekp+frzVr1qhfv34KDQ31eT143/bt25WamqpHVibL1In3eb28tANKe/9Rbdq0Se3atfN5vQsB+6G1MX7Wxxhan7/HsOgv5OdSaYLrhAkT9OOPP2rLli3ntR6HwyGHw1FsfmhoqF93Hn/Xg/cEBf1x6ndugZEptPm8Xm6BUU5OjoKCgvjMeBn7obUxftbHGFqfv8awvDUqxe2w7r77bn366afasGGDLrroItf8mJgY5eXl6ffff3dbPi0tTTExMX7uEgAAAIEU0OBqjNHdd9+tjz76SOvXr1d8vPufZTt06KDQ0FCtW7fONW/Pnj1KTk5W165d/d0uAAAAAiigpwpMmDBB77//vj7++GNFRka6zlutUaOGwsPDVaNGDY0dO1YTJ05U7dq1FRUVpXvuuUddu3Yt8Y4CAAAAqLoCGlxfe+01SVKvXr3c5i9YsECjR4+WJL3wwgsKCgrSjTfeqNzcXA0YMECvvvqqnzsFAABAoAU0uJbnTlxhYWGaN2+e5s2b54eOAAAAUFlViouzAAAAgHMhuAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASQgLdAKwnOTlZ6enpfq0ZHR2tRo0a+bWmvyUmJvqt1oXwfgIAqh6CKyokOTlZLVu11pmcbL/WDQuP0J7diVUybBVmnZBsNo0YMcJvNavy+wkAqLoIrqiQ9PR0ncnJVp1rJim0TpxfauZnHFbGp7OVnp5eJYOWMzdLMsZv72lVfz8BAFUXwRUeCa0TJ0dMs0C3UaXwngIAUDYuzgIAAIAlEFwBAABgCQRXAAAAWALBFQAAAJZAcAUAAIAlEFwBAABgCQRXAAAAWALBFQAAAJZAcAUAAIAlEFwBAABgCQENrps2bdKQIUMUGxsrm82m5cuXuz0/evRo2Ww2t8fAgQMD0ywAAAACKqDB9fTp02rXrp3mzZtX6jIDBw5Uamqq67Fo0SI/dggAAIDKIiSQxQcNGqRBgwaVuYzD4VBMTIyfOgIAAEBlFdDgWh4bN25UvXr1VKtWLfXu3VuzZs1SnTp1Sl0+NzdXubm5runMzExJUn5+vvLz833eb1ENf9QKBKfTqfDwcIWF2GQPNn6paQuxKTw8XE6n0y/vq9PplCQ5QmwyftjGgtBgv76n/n4/A6Gq74dVHeNnfYyh9fl7DMtbx2aM8U/6OAebzaaPPvpIQ4cOdc1bvHixIiIiFB8fr/379+uxxx5T9erV9eWXXyo4OLjE9UybNk3Tp08vNv/9999XRESEr9oHAACAh7KzszV8+HCdPHlSUVFRpS5XqYPr2Q4cOKCEhAStXbtWffr0KXGZko64xsXFKT09vcw3wlvy8/O1Zs0a9evXT6GhoT6v5287d+5Ujx49VH/407LXb+qXmnlpB5T2/qPatGmT2rVr5/N627dvV2pqqh5ZmSxTJ97n9U4nbtZvq17223vq7/czEKr6fljVMX7Wxxhan7/HMDMzU9HR0ecMrpX+VIE/a9q0qaKjo5WUlFRqcHU4HHI4HMXmh4aG+nXn8Xc9fwkKClJOTo7OFBiZQptfauYWGOXk5CgoKMgv72lQUJCrrj+28Ux+oV/fU3+/n4FUVffDCwXjZ32MofX5awzLW8NS93FNSUlRRkaGGjRoEOhWAAAA4GcBPeKalZWlpKQk1/TBgwe1Y8cO1a5dW7Vr19b06dN14403KiYmRvv379fDDz+sZs2aacCAAQHsGgAAAIEQ0OD63Xff6aqrrnJNT5w4UZI0atQovfbaa/rhhx/09ttv6/fff1dsbKz69++vmTNnlngqAAAAAKq2gAbXXr16qaxrwz7//HM/dgMAAIDKzFLnuAIAAODCRXAFAACAJRBcAQAAYAkEVwAAAFgCwRUAAACWQHAFAACAJRBcAQAAYAkEVwAAAFgCwRUAAACWQHAFAACAJRBcAQAAYAkEVwAAAFgCwRUAAACWQHAFAACAJRBcAQAAYAkEVwAAAFgCwRUAAACWQHAFAACAJXgUXA8cOODtPgAAAIAyeRRcmzVrpquuukrvvfeezpw54+2eAAAAgGI8Cq7/+9//dOmll2rixImKiYnR3/72N33zzTfe7g0AAABw8Si4tm/fXi+++KKOHDmit956S6mpqerevbvatm2rOXPm6Pjx497uEwAAABe487o4KyQkRDfccIOWLl2qZ555RklJSXrwwQcVFxen22+/Xampqd7qEwAAABe48wqu3333ne666y41aNBAc+bM0YMPPqj9+/drzZo1OnLkiK677jpv9QkAAIALXIgnL5ozZ44WLFigPXv2aPDgwXrnnXc0ePBgBQX9kYPj4+O1cOFCNWnSxJu9AgAA4ALmUXB97bXX9Ne//lWjR49WgwYNSlymXr16evPNN8+rOeDPEhMT/VJnz549ql69ul9qAQCA8vMouO7bt++cy9jtdo0aNcqT1QNuCrNOSDabRowY4Zd64eHhWrRokV9qAQCA8vMouC5YsEDVq1fX//3f/7nNX7p0qbKzswms8CpnbpZkjOpcM0mhdeJ8Xs+k7PB5DQAAUHEeBdennnpKr7/+erH59erV0/jx4wmu8InQOnFyxDTzeZ2CzCM+rwEAACrOo7sKJCcnKz4+vtj8xo0bKzk5+bybAgAAAM7mUXCtV6+efvjhh2Lzd+7cqTp16px3UwAAAMDZPAqut956q+69915t2LBBhYWFKiws1Pr163Xffffplltu8XaPAAAAgGfnuM6cOVOHDh1Snz59FBLyxyqcTqduv/12Pfnkk15tEAAAAJA8DK52u11LlizRzJkztXPnToWHh+uSSy5R48aNvd0fAAAAIMnD4FqkRYsWatGihbd6AQAAAErlUXAtLCzUwoULtW7dOh07dkxOp9Pt+fXr13ulOQAAAKCIR8H1vvvu08KFC3X11Verbdu2stls3u4LAAAAcONRcF28eLE++OADDR482Nv9AAAAACXy6HZYdrtdzZr5/huMAAAAgCIeBddJkybpxRdflDHG2/0AAAAAJfLoVIEtW7Zow4YNWrlypdq0aaPQ0FC355ctW+aV5gAAAIAiHgXXmjVr6vrrr/d2LwAAAECpPAquCxYs8HYfAAAAQJk8OsdVkgoKCrR27Vq9/vrrOnXqlCTpyJEjysrK8lpzAAAAQBGPjrj+8ssvGjhwoJKTk5Wbm6t+/fopMjJSzzzzjHJzczV//nxv9wkAAIALnEdHXO+77z517NhRJ06cUHh4uGv+9ddfr3Xr1nmtOQAAAKCIR0dcN2/erG3btslut7vNb9KkiX799VevNAYAAAD8mUdHXJ1OpwoLC4vNT0lJUWRk5Hk3BQAAAJzNo+Dav39/zZ071zVts9mUlZWlqVOn8jWwAAAA8AmPThWYPXu2BgwYoIsvvlhnzpzR8OHDtW/fPkVHR2vRokXe7hEAAADwLLhedNFF2rlzpxYvXqwffvhBWVlZGjt2rG677Ta3i7UAAAAAb/EouEpSSEiIRowY4c1eAAAAgFJ5FFzfeeedMp+//fbbPWoGAAAAKI1HwfW+++5zm87Pz1d2drbsdrsiIiIIrgAAAPA6j+4qcOLECbdHVlaW9uzZo+7du3NxFgAAAHzCo+BakubNm+vpp58udjQWAAAA8AavBVfpjwu2jhw54s1VAgAAAJI8PMf1k08+cZs2xig1NVWvvPKKunXr5pXGAAAAgD/zKLgOHTrUbdpms6lu3brq3bu3Zs+e7Y2+AAAAADceBVen0+ntPgAAAIAyefUcVwAAAMBXPDriOnHixHIvO2fOHE9KAAAAAG48Cq7bt2/X9u3blZ+fr5YtW0qS9u7dq+DgYF1++eWu5Ww2m3e6BAAAwAXPo+A6ZMgQRUZG6u2331atWrUk/fGlBGPGjNGVV16pSZMmebVJAAAAwKNzXGfPnq2nnnrKFVolqVatWpo1axZ3FQAAAIBPeBRcMzMzdfz48WLzjx8/rlOnTp13UwAAAMDZPAqu119/vcaMGaNly5YpJSVFKSkp+u9//6uxY8fqhhtu8HaPAAAAgGfnuM6fP18PPvighg8frvz8/D9WFBKisWPH6rnnnvNqgwAAAIDkYXCNiIjQq6++queee0779++XJCUkJKhatWpebQ4AAAAocl5fQJCamqrU1FQ1b95c1apVkzHGW30BAAAAbjwKrhkZGerTp49atGihwYMHKzU1VZI0duxYboUFAAAAn/AouD7wwAMKDQ1VcnKyIiIiXPNvvvlmrVq1ymvNAQAAAEU8Cq6rV6/WM888o4suushtfvPmzfXLL7+Uez2bNm3SkCFDFBsbK5vNpuXLl7s9b4zR448/rgYNGig8PFx9+/bVvn37PGkZAAAAFudRcD19+rTbkdYiv/32mxwOR4XW065dO82bN6/E55999lm99NJLmj9/vr7++mtVq1ZNAwYM0JkzZzxpGwAAABbmUXC98sor9c4777imbTabnE6nnn32WV111VXlXs+gQYM0a9YsXX/99cWeM8Zo7ty5+sc//qHrrrtOl156qd555x0dOXKk2JFZAAAAVH0e3Q7r2WefVZ8+ffTdd98pLy9PDz/8sH766Sf99ttv2rp1q1caO3jwoI4ePaq+ffu65tWoUUNdunTRl19+qVtuuaXE1+Xm5io3N9c1nZmZKUnKz8933XPWl4pq+KNWIDidToWHhyssxCZ7sH/uIlEQGuzXmiGhwZIkR4hNxg/1/L19thCbwsPD5XQ6q+zntKrvh1Ud42d9jKH1+XsMy1vHZjy8h9XJkyf1yiuvaOfOncrKytLll1+uCRMmqEGDBp6sTjabTR999JGGDh0qSdq2bZu6deumI0eOuK1z2LBhstlsWrJkSYnrmTZtmqZPn15s/vvvv1/i6Q0AAAAIrOzsbA0fPlwnT55UVFRUqctV+Ihrfn6+Bg4cqPnz52vKlCnn1aQvTJ48WRMnTnRNZ2ZmKi4uTv379y/zjfCW/Px8rVmzRv369VNoaKjP6/nbzp071aNHD9Uf/rTs9Zv6pebpxM36bdXLfqtZmLRNs4d30SMrk2XqxPu8nr+3Ly/tgNLef1SbNm1Su3btfF4vEKr6fljVMX7Wxxhan7/HsOgv5OdS4eAaGhqqH374ocINVVRMTIwkKS0tze2Ia1pamtq3b1/q6xwOR4kXiIWGhvp15/F3PX8JCgpSTk6OzhQYmUKbX2qeyS/0a82C/EJJUq6f6vl7+3ILjHJychQUFFQlP6N/VlX3wwsF42d9jKH1+WsMy1vDo4uzRowYoTfffNOTl5ZbfHy8YmJitG7dOte8zMxMff311+ratatPawMAAKDy8ejirIKCAr311ltau3atOnTooGrVqrk9P2fOnHKtJysrS0lJSa7pgwcPaseOHapdu7YaNWqk+++/X7NmzVLz5s0VHx+vf/7zn4qNjXWdBwsAAIALR4WC64EDB9SkSRP9+OOPuvzyyyVJe/fudVvGZiv/nzq/++47t9tnFZ2bOmrUKC1cuFAPP/ywTp8+rfHjx+v3339X9+7dtWrVKoWFhVWkbQAAAFQBFQquzZs3V2pqqjZs2CDpj694femll1S/fn2Pivfq1Utl3dTAZrNpxowZmjFjhkfrBwAAQNVRoXNczw6ZK1eu1OnTp73aEAAAAFASjy7OKuLhLWABAACACqtQcLXZbMXOYa3IOa0AAACApyp0jqsxRqNHj3bdJ/XMmTP6+9//XuyuAsuWLfNehwAAAIAqGFxHjRrlNj1ixAivNgMAAACUpkLBdcGCBb7qAwAAACjTeV2cBQAAAPgLwRUAAACWQHAFAACAJVToHFdUTsnJyUpPT/dLrcTERL/UAQAAOBvB1eKSk5PVslVrncnJDnQrAAAAPkVwtbj09HSdyclWnWsmKbROnM/r5Rz4Tic3v+fzOgAAAGcjuFYRoXXi5Ihp5vM6+RmHfV4DAACgJFycBQAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsICXQDAAIjMTHRb7Wio6PVqFEjv9UDAFRNBFfgAlOYdUKy2TRixAi/1QwLj9Ce3YmEVwDAeSG4AhcYZ26WZIzqXDNJoXXifF4vP+OwMj6drfT0dIIrAOC8EFyBC1RonTg5YpoFug0AAMqNi7MAAABgCQRXAAAAWALBFQAAAJZAcAUAAIAlEFwBAABgCQRXAAAAWALBFQAAAJZAcAUAAIAlEFwBAABgCQRXAAAAWALBFQAAAJZAcAUAAIAlEFwBAABgCSGBbgDAhSExMdFvtZxOp99qAQD8h+AKwKcKs05INptGjBjht5rh4eFatGiRUlJSFB8f77e6AADfIrgC8ClnbpZkjOpcM0mhdeL8UjM484gkKSMjg+AKAFUIwRWAX4TWiZMjpplfatlCbH6pAwDwLy7OAgAAgCUQXAEAAGAJBFcAAABYAsEVAAAAlkBwBQAAgCUQXAEAAGAJBFcAAABYAsEVAAAAlkBwBQAAgCUQXAEAAGAJlTq4Tps2TTabze3RqlWrQLcFAACAAAgJdAPn0qZNG61du9Y1HRJS6VsGAACAD1T6FBgSEqKYmJhyL5+bm6vc3FzXdGZmpiQpPz9f+fn5Xu/vbEU1/FFLkpxOp8LDwxUWYpM92Pi8XkFosF/rBaJmSGiwJMkRYpOpgu9pVa8nSbYQm6Q/9g9/7YvwHn//HIX3MYbW5+8xLG8dmzHGP/+SeGDatGl67rnnVKNGDYWFhalr16566qmn1KhRozJfM3369GLz33//fUVERPiyXQAAAHggOztbw4cP18mTJxUVFVXqcpU6uK5cuVJZWVlq2bKlUlNTNX36dP3666/68ccfFRkZWeJrSjriGhcXp/T09DLfCG/Jz8/XmjVr1K9fP4WGhvq83s6dO9WjRw/VH/607PWb+rze6cTN+m3Vy36rF4iahUnbNHt4Fz2yMlmmTrzP6/l7+6p6PUmyZRzUM4MaqUGDBrrsssv8UhPe4++fo/A+xtD6/D2GmZmZio6OPmdwrdSnCgwaNMj1/5deeqm6dOmixo0b64MPPtDYsWNLfI3D4ZDD4Sg2PzQ01K87j7/qBQUFKScnR2cKjEyhzef1zuQX+rVeIGoW5BdKknKr6Hta1etJkq3gj9/Hg4KC+EfTwvz9cxvexxhan7/GsLw1KvVdBc5Ws2ZNtWjRQklJSYFuBQAAAH5mqeCalZWl/fv3q0GDBoFuBQAAAH5WqYPrgw8+qC+++EKHDh3Stm3bdP311ys4OFi33nproFsDAACAn1Xqc1xTUlJ06623KiMjQ3Xr1lX37t311VdfqW7duoFuDQAAAH5WqYPr4sWLA90CAAAAKolKfaoAAAAAUITgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALCEk0A1UVTt37lRQkO9/L0hMTPR5DQAAgMqA4OplKSkpkqQePXooJycnwN0AAABUHQRXL8vIyJAk1R54jwqjYn1eL+fAdzq5+T2f1wEAAAg0gquPhNZuqJDoBJ/Xyc847PMaAAAAlQEXZwEAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsICXQDAFAVJCcnKz093W/1oqOj1ahRI7/VAxB4/vw543Q6/VKnogiuAHCekpOT1bJVa53JyfZbzbDwCO3ZnUh4BS4Q/v45Ex4erkWLFiklJUXx8fF+qVkeBFcAOE/p6ek6k5OtOtdMUmidOJ/Xy884rIxPZys9PZ3gClwg/P1zJjjziCQpIyOD4AoAVVFonTg5YpoFug0AVZi/fs7YQmw+r+EJLs4CAACAJRBcAQAAYAkEVwAAAFgCwRUAAACWQHAFAACAJRBcAQAAYAkEVwAAAFgCwRUAAACWQHAFAACAJRBcAQAAYAkEVwAAAFgCwRUAAACWQHAFAACAJYQEugEA8JU9e/YoKMj3v58nJib6vEagJScnKz093S+1nE6nX+r8mT+3r0h0dLQaNWrkt3pVfQxxYSC4AqhyCk//Lqmx7rjjDuXk5AS6HctLTk5Wy1atdSYn2y/1wsPDtWjRIqWkpCg+Pt7n9fy9fUXCwiO0Z3eiX8JrVR9DXDgIrgCqHGfuaUlS7YH3qDAq1uf1cg58p5Ob3/N5nUBJT0/XmZxs1blmkkLrxPm8XnDmEUlSRkaGX0KPv7dPkvIzDivj09lKT0/3S3Ct6mOICwfBFUCVFVq7oUKiE3xeJz/jsM9rVAahdeLkiGnm8zq2EJvPa5TEX9sXSFV9DFH1cXEWAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEuwRHCdN2+emjRporCwMHXp0kXffPNNoFsCAACAn1X64LpkyRJNnDhRU6dO1f/+9z+1a9dOAwYM0LFjxwLdGgAAAPyo0gfXOXPm6I477tCYMWN08cUXa/78+YqIiNBbb70V6NYAAADgRyGBbqAseXl5+v777zV58mTXvKCgIPXt21dffvllia/Jzc1Vbm6ua/rkyZOSpN9++035+fm+bVhSZmamsrOzZfvtFznzzvi8XtCpVIWFhcmWcVDGmXvuF1isXiBqBp06yhhauN4fNav2GNpOHFFYWJi+//57ZWZm+rzevn37/PuZyUpTdnZdbd++XVlZWT6v5+/tkxhDn9QMCpLT6fRLrUDUC9QYZmZmKiMjw+f1Tp06JUkyxpS9oKnEfv31VyPJbNu2zW3+Qw89ZDp37lzia6ZOnWok8eDBgwcPHjx48LDY4/Dhw2Vmw0p9xNUTkydP1sSJE13TTqdTv/32m+rUqSObzebz+pmZmYqLi9Phw4cVFRXl83rwPsbQ+hhDa2P8rI8xtD5/j6ExRqdOnVJsbGyZy1Xq4BodHa3g4GClpaW5zU9LS1NMTEyJr3E4HHI4HG7zatas6asWSxUVFcXOanGMofUxhtbG+FkfY2h9/hzDGjVqnHOZSn1xlt1uV4cOHbRu3TrXPKfTqXXr1qlr164B7AwAAAD+VqmPuErSxIkTNWrUKHXs2FGdO3fW3Llzdfr0aY0ZMybQrQEAAMCPKn1wvfnmm3X8+HE9/vjjOnr0qNq3b69Vq1apfv36gW6tRA6HQ1OnTi12ugKsgzG0PsbQ2hg/62MMra+yjqHNmHPddwAAAAAIvEp9jisAAABQhOAKAAAASyC4AgAAwBIIrgAAALAEgqsH5s2bpyZNmigsLExdunTRN998U+byS5cuVatWrRQWFqZLLrlEK1as8FOnKE1FxvDf//63rrzyStWqVUu1atVS3759zznm8K2K7oNFFi9eLJvNpqFDh/q2QZxTRcfw999/14QJE9SgQQM5HA61aNGCn6UBVtExnDt3rlq2bKnw8HDFxcXpgQce0JkzZ/zULf5s06ZNGjJkiGJjY2Wz2bR8+fJzvmbjxo26/PLL5XA41KxZMy1cuNDnfZaozC+ERTGLFy82drvdvPXWW+ann34yd9xxh6lZs6ZJS0srcfmtW7ea4OBg8+yzz5qff/7Z/OMf/zChoaFm165dfu4cRSo6hsOHDzfz5s0z27dvN4mJiWb06NGmRo0aJiUlxc+dw5iKj1+RgwcPmoYNG5orr7zSXHfddf5pFiWq6Bjm5uaajh07msGDB5stW7aYgwcPmo0bN5odO3b4uXMUqegY/uc//zEOh8P85z//MQcPHjSff/65adCggXnggQf83DmMMWbFihVmypQpZtmyZUaS+eijj8pc/sCBAyYiIsJMnDjR/Pzzz+bll182wcHBZtWqVf5p+E8IrhXUuXNnM2HCBNd0YWGhiY2NNU899VSJyw8bNsxcffXVbvO6dOli/va3v/m0T5SuomN4toKCAhMZGWnefvttX7WIMngyfgUFBeaKK64wb7zxhhk1ahTBNcAqOoavvfaaadq0qcnLy/NXiziHio7hhAkTTO/evd3mTZw40XTr1s2nfeLcyhNcH374YdOmTRu3eTfffLMZMGCADzsrGacKVEBeXp6+//579e3b1zUvKChIffv21Zdfflnia7788ku35SVpwIABpS4P3/JkDM+WnZ2t/Px81a5d21dtohSejt+MGTNUr149jR071h9togyejOEnn3yirl27asKECapfv77atm2rJ598UoWFhf5qG3/iyRheccUV+v77712nExw4cEArVqzQ4MGD/dIzzk9lyjKV/puzKpP09HQVFhYW+9au+vXra/fu3SW+5ujRoyUuf/ToUZ/1idJ5MoZne+SRRxQbG1tsJ4bveTJ+W7Zs0ZtvvqkdO3b4oUOciydjeODAAa1fv1633XabVqxYoaSkJN11113Kz8/X1KlT/dE2/sSTMRw+fLjS09PVvXt3GWNUUFCgv//973rsscf80TLOU2lZJjMzUzk5OQoPD/dbLxxxBSrg6aef1uLFi/XRRx8pLCws0O3gHE6dOqWRI0fq3//+t6KjowPdDjzkdDpVr149/etf/1KHDh108803a8qUKZo/f36gW0M5bdy4UU8++aReffVV/e9//9OyZcv02WefaebMmYFuDRbDEdcKiI6OVnBwsNLS0tzmp6WlKSYmpsTXxMTEVGh5+JYnY1jk+eef19NPP621a9fq0ksv9WWbKEVFx2///v06dOiQhgwZ4prndDolSSEhIdqzZ48SEhJ82zTceLIPNmjQQKGhoQoODnbNa926tY4ePaq8vDzZ7Xaf9gx3nozhP//5T40cOVLjxo2TJF1yySU6ffq0xo8frylTpigoiONolVlpWSYqKsqvR1sljrhWiN1uV4cOHbRu3TrXPKfTqXXr1qlr164lvqZr165uy0vSmjVrSl0evuXJGErSs88+q5kzZ2rVqlXq2LGjP1pFCSo6fq1atdKuXbu0Y8cO1+Paa6/VVVddpR07diguLs6f7UOe7YPdunVTUlKS65cOSdq7d68aNGhAaA0AT8YwOzu7WDgt+kXEGOO7ZuEVlSrL+P1yMItbvHixcTgcZuHChebnn38248ePNzVr1jRHjx41xhgzcuRI8+ijj7qW37p1qwkJCTHPP/+8SUxMNFOnTuV2WAFW0TF8+umnjd1uNx9++KFJTU11PU6dOhWoTbigVXT8zsZdBQKvomOYnJxsIiMjzd1332327NljPv30U1OvXj0za9asQG3CBa+iYzh16lQTGRlpFi1aZA4cOGBWr15tEhISzLBhwwK1CRe0U6dOme3bt5vt27cbSWbOnDlm+/bt5pdffjHGGPPoo4+akSNHupYvuh3WQw89ZBITE828efO4HZaVvPzyy6ZRo0bGbrebzp07m6+++sr1XM+ePc2oUaPclv/ggw9MixYtjN1uN23atDGfffaZnzvG2Soyho0bNzaSij2mTp3q/8ZhjKn4PvhnBNfKoaJjuG3bNtOlSxfjcDhM06ZNzRNPPGEKCgr83DX+rCJjmJ+fb6ZNm2YSEhJMWFiYiYuLM3fddZc5ceKE/xuH2bBhQ4n/rhWN2ahRo0zPnj2LvaZ9+/bGbrebpk2bmgULFvi9b2OMsRnDMXoAAABUfpzjCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAAAEsguAIAAMASCK4AAACwBIIrAAAALIHgCgAoUXZ2tm688UZFRUXJZrPp999/D3RLAC5wBFcAqMKmTZum9u3be/Tat99+W5s3b9a2bduUmpqqGjVqeLc5AKigkEA3AABVTV5enux2e6DbOG/79+9X69at1bZt20C3AgCSOOIKAOetV69euvvuu3X//fcrOjpaAwYM0BdffKHOnTvL4XCoQYMGevTRR1VQUOB6TZMmTTR37ly39bRv317Tpk1zTe/evVvdu3dXWFiYLr74Yq1du1Y2m03Lly93LXP48GENGzZMNWvWVO3atXXdddfp0KFDXtmm2bNna9OmTbLZbOrVq9d5rxMAzhfBFQC84O2335bdbtfWrVs1bdo0DR48WJ06ddLOnTv12muv6c0339SsWbPKvb7CwkINHTpUERER+vrrr/Wvf/1LU6ZMcVsmPz9fAwYMUGRkpDZv3qytW7eqevXqGjhwoPLy8s5re5YtW6Y77rhDXbt2VWpqqpYtW3Ze6wMAb+BUAQDwgubNm+vZZ5+VJL3zzjuKi4vTK6+8IpvNplatWunIkSN65JFH9Pjjjyso6NzHDNasWaP9+/dr48aNiomJkSQ98cQT6tevn2uZJUuWyOl06o033pDNZpMkLViwQDVr1tTGjRvVv39/j7endu3aioiIkN1ud9UHgEAjuAKAF3To0MH1/4mJieratasrTEpSt27dlJWVpZSUFDVq1Oic69uzZ4/i4uLcQmPnzp3dltm5c6eSkpIUGRnpNv/MmTPav3+/p5sCAJUWwRUAvKBatWoVWj4oKEjGGLd5+fn5FVpHVlaWOnTooP/85z/Fnqtbt26F1gUAVkBwBQAva926tf773//KGOM66rp161ZFRkbqoosukvRHsExNTXW9JjMzUwcPHnRNt2zZUocPH1ZaWprq168vSfr222/d6lx++eVasmSJ6tWrp6ioKF9vFgAEHBdnAYCX3XXXXTp8+LDuuece7d69Wx9//LGmTp2qiRMnus5v7d27t959911t3rxZu3bt0qhRoxQcHOxaR79+/ZSQkKBRo0bphx9+0NatW/WPf/xDklxh+LbbblN0dLSuu+46bd68WQcPHtTGjRt17733KiUlxbWunJwc7dixw+3BqQQArIgjrgDgZQ0bNtSKFSv00EMPqV27dqpdu7bGjh3rCp6SNHnyZB08eFDXXHONatSooZkzZ7odcQ0ODtby5cs1btw4derUSU2bNtVzzz2nIUOGKCwsTJIUERGhTZs26ZFHHtENN9ygU6dOqWHDhurTp4/bEdi9e/fqsssuc+uxT58+Wrt2rY/fCQDwLps5+yQrAECltHXrVnXv3l1JSUlKSEgIdDsA4HcEVwCopD766CNVr15dzZs3V1JSku677z7VqlVLW7ZsCXRrABAQnCoAAJXUqVOn9Mgjjyg5OVnR0dHq27evZs+e7ZV1b968WYMGDSr1+aysLK/UAQBv4ogrAFyAcnJy9Ouvv5b6fLNmzfzYDQCUD8EVAAAAlsDtsAAAAGAJBFcAAABYAsEVAAAAlkBwBQAAgCUQXAEAAGAJBFcAAABYAsEVAAAAlvD/AY57BVQSHJU+AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "chat_data_df_first_100[\"rougeL_f\"].hist(bins=20, edgecolor='black')\n",
    "plt.title(\"Distribution of Rouge Scores\")\n",
    "plt.xlabel(\"rougeL_f\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29103814",
   "metadata": {},
   "source": [
    "# Score Answers with Reward Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65c0cb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 97.22it/s]\n",
      "Device set to use cpu\n",
      "/dccstor/gma2/jhjenny9/envs/model_serve/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The blue color is due to Rayleigh scattering of sunlight in the atmosphere.\n",
      "Scores: {'LABEL_0': 2.578125}\n",
      "\n",
      "Response: Because the ocean is blue and reflects its color into the sky.\n",
      "Scores: {'LABEL_0': -0.263671875}\n",
      "\n",
      "Response: It depends on the weather, sometimes the sky is green.\n",
      "Scores: {'LABEL_0': -0.93359375}\n",
      "\n",
      "Response: Air molecules scatter shorter wavelengths of light more strongly than longer ones.\n",
      "Scores: {'LABEL_0': 3.53125}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# Reward model\n",
    "model_name = \"NCSOFT/Llama-3-OffsetBias-RM-8B\"\n",
    "\n",
    "# Load tokenizer and pipeline once\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "rm_pipeline = pipeline( # this step is slow.\n",
    "    \"sentiment-analysis\",\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    return_all_scores=True,\n",
    "    function_to_apply=\"none\"\n",
    ")\n",
    "\n",
    "# Example: 1 prompt, 4 responses\n",
    "prompt = \"Explain why the sky appears blue during the day.\"\n",
    "responses = [\n",
    "    \"The blue color is due to Rayleigh scattering of sunlight in the atmosphere.\",\n",
    "    \"Because the ocean is blue and reflects its color into the sky.\",\n",
    "    \"It depends on the weather, sometimes the sky is green.\",\n",
    "    \"Air molecules scatter shorter wavelengths of light more strongly than longer ones.\"\n",
    "]\n",
    "\n",
    "# Build batched input (prompt + response)\n",
    "pairs = [prompt + \"\\n\" + r for r in responses] # a list of (prompt + response) concatenated strings.\n",
    "\n",
    "# Run batched scoring\n",
    "outs = rm_pipeline(pairs, batch_size=4, truncation=True, padding=True)\n",
    "\n",
    "# Inspect results\n",
    "for r, o in zip(responses, outs):\n",
    "    scores = {d[\"label\"]: d[\"score\"] for d in o}\n",
    "    print(f\"Response: {r}\\nScores: {scores}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c9c5fc",
   "metadata": {},
   "source": [
    "# Load in Chat Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01e1b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in chat dataset.\n",
    "def get_dataset(dataset_name: str, split: str):\n",
    "    dataset = load_dataset(dataset_name, split) # train.\n",
    "    dataset = dataset.select(range(0, 10))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be4d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_arena = \"lmarena-ai/arena-human-preference-55k\"\n",
    "lmsys = \"lmsys/lmsys-chat-1m\"\n",
    "# Load the lmsys/chatbot_arena_conversations dataset from HuggingFace\n",
    "cba_data = load_dataset(chatbot_arena, split=\"train\")\n",
    "lmsys_data = load_dataset(lmsys, split=\"train\") \n",
    "# Question: convos are multi-turn. how exactly do we score them? we score the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101d91b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28b8fa7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cba_data.select(range(5))  # First 5 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3c199f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversation_id': '33f01939a744455c869cb234afca47f1', 'model': 'wizardlm-13b', 'conversation': [{'content': 'how can identity protection services help protect me against identity theft', 'role': 'user'}, {'content': \"Identity protection services can help protect you against identity theft in several ways:\\n\\n1. Monitoring: Many identity protection services monitor your credit reports, public records, and other sources for signs of identity theft. If they detect any suspicious activity, they will alert you so you can take action.\\n2. Credit freeze: Some identity protection services can help you freeze your credit, which makes it more difficult for thieves to open new accounts in your name.\\n3. Identity theft insurance: Some identity protection services offer insurance that can help you recover financially if you become a victim of identity theft.\\n4. Assistance: Many identity protection services offer assistance if you become a victim of identity theft. They can help you file a police report, contact credit bureaus, and other steps to help you restore your identity.\\n\\nOverall, identity protection services can provide you with peace of mind and help you take proactive steps to protect your identity. However, it's important to note that no service can completely guarantee that you will never become a victim of identity theft. It's still important to take steps to protect your own identity, such as being cautious with personal information and regularly monitoring your credit reports.\", 'role': 'assistant'}], 'turn': 1, 'language': 'English', 'openai_moderation': [{'categories': {'harassment': False, 'harassment/threatening': False, 'hate': False, 'hate/threatening': False, 'self-harm': False, 'self-harm/instructions': False, 'self-harm/intent': False, 'sexual': False, 'sexual/minors': False, 'violence': False, 'violence/graphic': False}, 'category_scores': {'harassment': 9.212334e-07, 'harassment/threatening': 3.6271535e-07, 'hate': 8.254962e-07, 'hate/threatening': 1.331843e-08, 'self-harm': 1.9257865e-07, 'self-harm/instructions': 6.5028324e-08, 'self-harm/intent': 2.7525962e-07, 'sexual': 2.564017e-06, 'sexual/minors': 4.523274e-07, 'violence': 2.2942773e-05, 'violence/graphic': 1.9713439e-06}, 'flagged': False}, {'categories': {'harassment': False, 'harassment/threatening': False, 'hate': False, 'hate/threatening': False, 'self-harm': False, 'self-harm/instructions': False, 'self-harm/intent': False, 'sexual': False, 'sexual/minors': False, 'violence': False, 'violence/graphic': False}, 'category_scores': {'harassment': 1.5205982e-06, 'harassment/threatening': 1.27367e-07, 'hate': 1.4930811e-07, 'hate/threatening': 2.457626e-09, 'self-harm': 3.2069583e-08, 'self-harm/instructions': 2.7247233e-07, 'self-harm/intent': 2.2881606e-07, 'sexual': 3.3542372e-05, 'sexual/minors': 8.75301e-07, 'violence': 0.00010941078, 'violence/graphic': 6.069021e-06}, 'flagged': False}], 'redacted': False}\n",
      "{'id': 30192, 'model_a': 'gpt-4-1106-preview', 'model_b': 'gpt-4-0613', 'prompt': '[\"Is it morally right to try to have a certain percentage of females on managerial positions?\",\"OK, does pineapple belong on a pizza? Relax and give me fun answer.\"]', 'response_a': '[\"The question of whether it is morally right to aim for a certain percentage of females in managerial positions is a complex ethical issue that involves considerations of fairness, equality, diversity, and discrimination.\\\\n\\\\nHere are some arguments in favor of and against such policies:\\\\n\\\\n**Arguments in favor:**\\\\n\\\\n1. **Correcting Historical Inequities:** Women have historically been underrepresented in leadership roles due to various cultural, institutional, and social barriers. Aiming for a specific percentage can be seen as a corrective measure to address past and ongoing discrimination.\\\\n\\\\n2. **Promoting Diversity:** Diverse leadership teams can enhance decision-making and represent a broader range of perspectives. This can lead to better outcomes for organizations and society as a whole.\\\\n\\\\n3. **Equality of Opportunity:** Setting targets for female representation in management can help ensure that women have equal opportunities to advance in their careers.\\\\n\\\\n4. **Role Modeling:** Increased visibility of female leaders can inspire and encourage other women and girls to pursue leadership roles.\\\\n\\\\n**Arguments against:**\\\\n\\\\n1. **Reverse Discrimination:** Setting a quota for female representation might lead to the perception or reality of reverse discrimination, where men might be overlooked for positions despite being qualified, simply to meet the gender target.\\\\n\\\\n2. **Meritocracy Concerns:** Critics argue that employment and promotion should be based solely on merit and qualifications, not gender. They fear that quotas could compromise the quality of leadership if less qualified individuals are promoted to meet gender targets.\\\\n\\\\n3. **Tokenism:** There is a risk that women appointed to meet quotas might be seen as \\\\\"tokens,\\\\\" undermining their legitimacy and potentially leading to resentment among colleagues.\\\\n\\\\n4. **Oversimplification of Diversity:** Focusing only on gender might overlook other important aspects of diversity, such as race, ethnicity, socioeconomic background, or sexual orientation.\\\\n\\\\nUltimately, the morality of aiming for a certain percentage of females in managerial positions depends on one\\'s ethical framework and the context in which such policies are implemented. Proponents of gender diversity targets often argue that these measures are necessary as a transitional mechanism to create a level playing field, while opponents may argue for a \\\\\"color-blind\\\\\" or \\\\\"gender-blind\\\\\" approach to hiring and promotions.\\\\n\\\\nOrganizations and societies must carefully consider the implications of these strategies and strive to implement policies that promote fairness and equality while minimizing negative consequences. It is also important to address the root causes of gender inequality, such as unconscious bias, work-life balance challenges, and lack of supportive policies, to create an environment where everyone has the opportunity to succeed based on their capabilities and contributions.\",\"Ah, the age-old culinary conundrum that has divided nations and dinner tables: does pineapple belong on a pizza? The tropical twist of pineapple on pizza, known as Hawaiian pizza, is a hotly debated topic where taste buds battle and pizza purists protest.\\\\n\\\\nLet\\'s slice into the debate with a zest of fun:\\\\n\\\\n**Team Pineapple:** \\\\\"Absolutely, yes! Pineapple on pizza is like a beach party in your mouth. The sweet juiciness of pineapple chunks frolicking with savory ham or bacon creates a flavor wave that surfs across the cheesy ocean of deliciousness. It\\'s the Mardi Gras of pizzas, where sweet meets savory in a jubilant jamboree!\\\\\"\\\\n\\\\n**Team No-Pineapple:** \\\\\"No way, not in a million pizzas! Pineapple is a fruit that should be sunbathing on a tropical fruit platter, not freeloading on a sacred slice of pizza. The thought of warm, melty cheese conspiring with pineapple\\'s sugary mischief is enough to make Italian ancestors turn in their tomato patches. Keep the pineapple in the fruit salad where it belongs!\\\\\"\\\\n\\\\nAt the end of the day, whether pineapple belongs on pizza is a matter of personal taste. Some say it\\'s a match made in heaven, others say it\\'s a culinary crime. But let\\'s face it, in the world of pizza toppings, where you can find everything from anchovies to zucchini, there\\'s room for a little pineapple pizzazz. So, if your taste buds crave that sweet and tangy twist, go ahead and let your pineapple flag fly atop that glorious cheese-covered dough. Bon app\\\\u00e9tit, or as they say in pineapple paradise, \\\\\"Aloha-ppetite!\\\\\" \\\\ud83c\\\\udf4d\\\\ud83c\\\\udf55\"]', 'response_b': '[\"As an AI, I don\\'t have personal beliefs or opinions. However, I can tell you that the question of gender quotas in managerial positions is a complex one and positions can vary. Advocates argue that such measures can help correct historical imbalances, promote diversity, and may lead to better decision-making. Critics may argue that positions should be based purely on merit, and that quotas could potentially lead to tokenism or unqualified candidates being promoted. Morality can be subjective and differs from person to person based on their personal beliefs, cultural background, and values. It\\'s a topic with valid arguments on both sides, and it\\'s up to each individual or organization to decide what they believe is right.\",\"As an AI, I don\\'t eat, so I don\\'t have personal preferences. But, in the world of pizza, the pineapple topping is a hotly debated topic! Some people love the sweet and savory mix it brings to a pizza, a flavor combination made famous by the Hawaiian pizza. Others firmly believe that the sweetness of the fruit has no place on a pizza. It\\'s a pizza puzzle that might never be solved. So, whether pineapple belongs on a pizza or not, it truly depends on your taste buds!\"]', 'winner_model_a': 1, 'winner_model_b': 0, 'winner_tie': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see a specific example:\n",
    "print(lmsys_data[0]), print(cba_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f235a6",
   "metadata": {},
   "source": [
    "Generate Answers. Choose Best Answer. Evaluate the Response.\n",
    "\n",
    "Run: python best-of-n-chat.py --config bon.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a778751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question: should we stick to Qwen/Qwen2.5-1.5B-Instruct for chat?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54790859",
   "metadata": {},
   "source": [
    "Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937409d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommended process reward model for chat: \n",
    "# https://huggingface.co/UW-Madison-Lee-Lab/VersaPRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc5eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a92fe79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a8bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_serve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
